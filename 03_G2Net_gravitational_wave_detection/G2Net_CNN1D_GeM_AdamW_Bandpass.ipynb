{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "G2Net Gravitational Wave Detection\n",
        "==================================\n",
        "CNN-1D + GeM Pooling + Bandpass Filter\n",
        "--------------------------------------\n",
        "\n",
        "- [1 Install packages, imports and settings](#install-imports-settings)\n",
        "- [2 Definitions](#definitions)\n",
        "  - [2.1 Front-end: signal filter and dataset builder](#front-end)\n",
        "  - [2.2 CNN model and GeM layer](#cnn-model-gem-layer)\n",
        "  - [2.3 Training and inference functions](#training-inference-functions)\n",
        "- [3 Configuration](#configuration)\n",
        "- [4 Run training and inference](#run-training-inference)\n",
        "- [5 Offline inference from multiple runs of folds](#offline-inference)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1 Install packages, imports and settings <a class=\"anchor\" id=\"install-imports-settings\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8yb6r66B9hJ",
        "outputId": "8ac56094-46e4-4fe8-c522-f3837d294261"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.1 MB 4.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 133 kB 6.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 38.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 40.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 42.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 871 kB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -qU tensorflow_addons\n",
        "%pip install -qU gcsfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7nVySYUINxS",
        "outputId": "65e9fe10-84c3-46ab-b688-fdde60aafb5e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[LogicalDevice(name='/device:CPU:0', device_type='CPU'),\n",
              " LogicalDevice(name='/device:GPU:0', device_type='GPU')]"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import logging\n",
        "import json, joblib\n",
        "from datetime import datetime\n",
        "from collections import namedtuple\n",
        "from functools import partial\n",
        "\n",
        "# Numerical, stats and ML\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import dask.array as da\n",
        "from scipy import signal\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_addons as tfa\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
        "tf.get_logger().setLevel('WARNING')\n",
        "tf.config.list_logical_devices()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2 Definitions <a class=\"anchor\" id=\"definitions\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.1 Front-end: signal filter and dataset builder <a class=\"anchor\" id=\"front-end\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cn9JfGX4INxk"
      },
      "outputs": [],
      "source": [
        "class SignalFilter:\n",
        "    \"\"\"\n",
        "    Cell 33 of https://www.gw-openscience.org/LVT151012data/LOSC_Event_tutorial_LVT151012.html\n",
        "    https://scipy-cookbook.readthedocs.io/items/ButterworthBandpass.html\n",
        "    \"\"\"\n",
        "    def __init__(self, scaling: np.ndarray, filter_type: str='highpass',\n",
        "                 filter_order: int=5, f_lo: float=20.43, f_hi: float=None,\n",
        "                 sampling_rate: int=2048):\n",
        "        self.scaling = scaling\n",
        "        if filter_type.lower() == 'bandpass':\n",
        "            Wn = [f_lo, f_hi]\n",
        "            self.filter_window = signal.tukey(4096, 0.1)\n",
        "            self.filter_norm = np.sqrt((f_hi - f_lo) / (sampling_rate / 2))\n",
        "        elif filter_type.lower() == 'highpass':\n",
        "            Wn = f_lo\n",
        "            self.filter_window = 1.\n",
        "            self.filter_norm = 1.\n",
        "        else:\n",
        "            raise ValueError('Unknown filter type.')\n",
        "        self.filter = signal.butter(N=filter_order, Wn=Wn, btype=filter_type, output='sos', fs=sampling_rate)\n",
        "\n",
        "    def filt(self, X):\n",
        "        X = self.scaling * X * self.filter_window\n",
        "        X = signal.sosfilt(self.filter, X)\n",
        "        X *= self.filter_norm\n",
        "        return X\n",
        "\n",
        "class Data:\n",
        "    \"\"\"\n",
        "    Dataset builder.\n",
        "    \"\"\"\n",
        "    tfrec_format_train = {\n",
        "        \"wave\": tf.io.FixedLenFeature([], tf.string),\n",
        "        \"target\": tf.io.FixedLenFeature([], tf.int64),\n",
        "        \"wave_id\": tf.io.FixedLenFeature([], tf.string),\n",
        "    }\n",
        "\n",
        "    tfrec_format_test = {\n",
        "        \"wave\": tf.io.FixedLenFeature([], tf.string),\n",
        "        \"wave_id\": tf.io.FixedLenFeature([], tf.string)\n",
        "    }\n",
        "\n",
        "    AUTO = tf.data.AUTOTUNE\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "\n",
        "        # Data file paths\n",
        "        self.train_files = [config.data_path + f'train{i}.tfrecords' for i in range(20)]\n",
        "        self.test_files = [config.data_path + f'test{i}.tfrecords' for i in range(10)]\n",
        "\n",
        "        # Front-end signal filter\n",
        "        self.filter = SignalFilter(**config.filter)\n",
        "\n",
        "    def _preprocess(self, X, y, train_or_test=True):\n",
        "        \"\"\"\n",
        "        Preprocess a batch of data: scaling, filtering, transpose. Casting to tf.float32 is done in wrapper.\n",
        "        \"\"\"\n",
        "        X = X.numpy()\n",
        "        X = self.filter.filt(X)\n",
        "        X = np.transpose(X, axes=(0, 2, 1))\n",
        "        if train_or_test:\n",
        "            return X, y\n",
        "        else:\n",
        "            return X\n",
        "\n",
        "    def _preprocess_wrapper(self, train_or_test=True):\n",
        "        if train_or_test:\n",
        "            def wrapper(X, y):\n",
        "                return tf.py_function(\n",
        "                    self._preprocess,\n",
        "                    inp=[X, y], Tout=[tf.float32, tf.float32])\n",
        "        else:\n",
        "            def wrapper(X):\n",
        "                return tf.py_function(\n",
        "                    partial(self._preprocess, y=None, train_or_test=False),\n",
        "                    inp=[X], Tout=tf.float32)\n",
        "        return wrapper\n",
        "\n",
        "    def _decode_train(self, tfrecord):\n",
        "        tensor_dict = tf.io.parse_single_example(tfrecord, self.tfrec_format_train)\n",
        "        X = tf.reshape(tf.io.decode_raw(tensor_dict['wave'], tf.float64), (3, 4096))\n",
        "        y = tf.reshape(tf.cast(tensor_dict['target'], tf.float32), [1])\n",
        "        # sample_ids = tensor_dict['sample_id']\n",
        "        return X, y\n",
        "\n",
        "    def _decode_test(self, tfrecord):\n",
        "        tensor_dict = tf.io.parse_single_example(tfrecord, self.tfrec_format_test)\n",
        "        X = tf.reshape(tf.io.decode_raw(tensor_dict['wave'], tf.float64), (3, 4096))\n",
        "        return X\n",
        "\n",
        "    def get_dataset(self, train_or_test=True, shuffle=True, file_indices=None):\n",
        "        data_files = self.train_files if train_or_test else self.test_files\n",
        "        if file_indices is not None:\n",
        "            data_files = [data_files[i] for i in file_indices]\n",
        "        ds = tf.data.TFRecordDataset(  # do not interleave test data files with parallel reads\n",
        "            data_files, num_parallel_reads=self.AUTO if train_or_test else 1, compression_type=\"GZIP\")\n",
        "        if shuffle:\n",
        "            options = tf.data.Options()\n",
        "            options.experimental_deterministic = False\n",
        "            ds = ds.shuffle(self.config.shuffle_buf_size).with_options(options)\n",
        "        ds = ds.map(self._decode_train if train_or_test else self._decode_test, num_parallel_calls=self.AUTO)\n",
        "        ds = ds.batch(self.config.batch_size).map(self._preprocess_wrapper(train_or_test), num_parallel_calls=self.AUTO)\n",
        "        ds = ds.prefetch(self.AUTO)\n",
        "        return ds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.2 CNN model and GeM layer <a class=\"anchor\" id=\"cnn-model-gem-layer\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7CuG_KcINxq"
      },
      "outputs": [],
      "source": [
        "class GeM(keras.layers.Layer):\n",
        "    def __init__(self, pool_size, p=3, eps=1e-6, **kwargs):\n",
        "        super(GeM, self).__init__(**kwargs)\n",
        "        self.pool_size = pool_size\n",
        "        self.p = p\n",
        "        self.eps = eps\n",
        "\n",
        "    def call(self, x):\n",
        "        x = tf.math.maximum(x, self.eps)\n",
        "        x = tf.pow(x, self.p)\n",
        "        x = tf.nn.avg_pool(x, self.pool_size, self.pool_size, 'VALID')\n",
        "        x = tf.pow(x, 1./self.p)\n",
        "        return x\n",
        "\n",
        "def get_model():\n",
        "    \"\"\"\n",
        "    Modified from\n",
        "    https://journals.aps.org/prl/pdf/1check0.1103/PhysRevLett.120.141103\n",
        "    \"\"\"\n",
        "    model = keras.models.Sequential([\n",
        "        keras.layers.Conv1D(64, 64, padding='valid', input_shape=(4096, 3)),\n",
        "        keras.layers.BatchNormalization(),\n",
        "        keras.layers.Activation(tf.nn.silu),\n",
        "        \n",
        "        keras.layers.Conv1D(64, 32, padding='valid'),\n",
        "        GeM(pool_size=8),\n",
        "        keras.layers.BatchNormalization(),\n",
        "        keras.layers.Activation(tf.nn.silu),\n",
        "\n",
        "        keras.layers.Conv1D(128, 32, padding='valid'),\n",
        "        keras.layers.BatchNormalization(),\n",
        "        keras.layers.Activation(tf.nn.silu),\n",
        "        \n",
        "        keras.layers.Conv1D(128, 16, padding='valid'),\n",
        "        GeM(pool_size=6),\n",
        "        keras.layers.BatchNormalization(),\n",
        "        keras.layers.Activation(tf.nn.silu),\n",
        "\n",
        "        keras.layers.Conv1D(256, 16, padding='valid'),\n",
        "        keras.layers.BatchNormalization(),\n",
        "        \n",
        "        keras.layers.Conv1D(256, 16, padding='valid'),\n",
        "        GeM(pool_size=4),\n",
        "        keras.layers.BatchNormalization(),\n",
        "        keras.layers.Activation(tf.nn.silu),\n",
        "\n",
        "        keras.layers.Flatten(),\n",
        "        # keras.layers.GlobalAveragePooling1D(),\n",
        "\n",
        "        keras.layers.Dense(64),\n",
        "        keras.layers.BatchNormalization(),\n",
        "        keras.layers.Dropout(0.25),\n",
        "        keras.layers.Activation(tf.nn.silu),\n",
        "\n",
        "        keras.layers.Dense(64),\n",
        "        keras.layers.BatchNormalization(),\n",
        "        keras.layers.Dropout(0.25),\n",
        "        keras.layers.Activation(tf.nn.silu),\n",
        "\n",
        "        keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.3 Training and inference functions <a class=\"anchor\" id=\"training-inference-functions\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXNvjXOHINxs"
      },
      "outputs": [],
      "source": [
        "def seed_all(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LYpQzubIwkc"
      },
      "outputs": [],
      "source": [
        "def get_logger(\n",
        "    logger_name,\n",
        "    log_path=None,\n",
        "    file_level=logging.INFO,\n",
        "    stream_level=logging.INFO,\n",
        "):\n",
        "    if log_path is None and stream_level is None:\n",
        "        raise ValueError(\"Both file and stream logger is None.\")\n",
        "    logger = logging.getLogger(logger_name)\n",
        "    logger.setLevel(logging.INFO)\n",
        "    logger_format = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\")\n",
        "    # Add file handler\n",
        "    if log_path is not None and Path(log_path).expanduser().resolve().exists():\n",
        "        logger_file = (\n",
        "            Path(log_path).expanduser().resolve().joinpath(logger_name + \".log\")\n",
        "        )\n",
        "        if logger_file.exists():\n",
        "            logger_file.unlink()\n",
        "        fh = logging.FileHandler(logger_file)\n",
        "        fh.setLevel(file_level)\n",
        "        fh.setFormatter(logger_format)\n",
        "        logger.addHandler(fh)\n",
        "    # Add stream handler\n",
        "    if stream_level is not None:\n",
        "        sh = logging.StreamHandler()\n",
        "        sh.setLevel(stream_level)\n",
        "        sh.setFormatter(logger_format)\n",
        "        logger.addHandler(sh)\n",
        "    return logger\n",
        "\n",
        "class Config(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "def check_save_config(config: Config):\n",
        "    config.results_path = str(Path(config.results_parent_path).joinpath(config.name))\n",
        "    p = Path(config.results_path).expanduser().resolve()\n",
        "    try:\n",
        "        p.mkdir(parents=True, exist_ok=False)\n",
        "    except FileExistsError:\n",
        "        if any(p.iterdir()):\n",
        "            raise ValueError(\"Non-empty results directory.\")\n",
        "    joblib.dump(config, p.joinpath('config.pkl'))\n",
        "    attrs = {k: v for k, v in vars(config).items() if not k.startswith('__')}\n",
        "    with open(p.joinpath('config.json'), 'w') as f:  # Human readable JSON\n",
        "        try:\n",
        "            attrs['filter']['scaling'] = attrs['filter']['scaling'].tolist()\n",
        "        except AttributeError:\n",
        "            pass\n",
        "        json.dump(attrs, f)\n",
        "\n",
        "def train(config: Config, train_data, val_data, fold_k=None):\n",
        "    \"\"\"\n",
        "    Train a single fold of training dataset.\n",
        "    \"\"\"\n",
        "    checkpoint_path = Path(config.results_path).joinpath(\n",
        "        'checkpoint' + ('' if fold_k is None else f\"_fold{fold_k}\" ))\n",
        "    step = tf.Variable(0, trainable=False)\n",
        "    lr_schedule_class = getattr(tf.keras.optimizers.schedules, config.lr_schedule)\n",
        "    lr_schedule = lr_schedule_class(**config.lr_schedule_paras[config.lr_schedule])\n",
        "    optimizer_class = getattr(tfa.optimizers, config.optimizer)\n",
        "    optimizer = optimizer_class(learning_rate=1*lr_schedule(step), **config.optimizer_paras[config.optimizer]) \n",
        "    # weight_decay = lambda: cfg.optimizer['weight_decay'] * lr_schedule(step))\n",
        "\n",
        "    model = get_model()\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[keras.metrics.AUC(name='AUC')])\n",
        "    checkpoint_cb = ModelCheckpoint(checkpoint_path, monitor='val_AUC', verbose=0, save_best_only=True, mode='max')\n",
        "    history = model.fit(train_data, epochs=config.epochs, callbacks=[checkpoint_cb], validation_data=val_data)\n",
        "    pd.DataFrame(history.history).to_csv(checkpoint_path.joinpath('train_history.csv'))\n",
        "\n",
        "    # return best model and the checkpoint path\n",
        "    model = keras.models.load_model(checkpoint_path)\n",
        "    return model, checkpoint_path\n",
        "\n",
        "def get_score(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute ROC AUC score on tensors.\n",
        "    \"\"\"\n",
        "    auc = tf.keras.metrics.AUC()\n",
        "    auc.update_state(y_true, y_pred)\n",
        "    score = auc.result().numpy()\n",
        "    return score\n",
        "\n",
        "def oof_predict(model, val_data):\n",
        "    \"\"\"\n",
        "    Predict on the validation dataset of a single fold.\n",
        "    \"\"\"\n",
        "    val_y_true = []\n",
        "    val_y_pred = []\n",
        "    for X, y in val_data:\n",
        "        val_y_true.append(y)\n",
        "        val_y_pred.append(model(X))\n",
        "    val_y_true = tf.concat(val_y_true, axis=0)\n",
        "    val_y_pred = tf.concat(val_y_pred, axis=0)\n",
        "    val_score = get_score(val_y_true, val_y_pred)\n",
        "    return val_y_true, val_y_pred, val_score\n",
        "\n",
        "def make_inference(config, models, logger=None):\n",
        "    \"\"\"\n",
        "    Make inference on test dataset and create submission file.\n",
        "    \"\"\"\n",
        "    logger_func = logger.info if logger else print\n",
        "    data = Data(config)\n",
        "    test_data = data.get_dataset(train_or_test=False, shuffle=False)\n",
        "    test_preds = []\n",
        "    for k, model in zip(config.train_folds or range(config.K_fold), models):\n",
        "        logger_func(f\"Make inference by fold {k} model\")\n",
        "        y_pred = model.predict(test_data)\n",
        "        test_preds.append(y_pred)\n",
        "    test_preds = np.concatenate(test_preds, axis=1).mean(axis=1)\n",
        "    sample = pd.read_csv(config.data_path + 'sample_submission.csv')\n",
        "    test_preds_df = pd.DataFrame({'id': sample['id'].values, 'target': test_preds})\n",
        "    p_submission = Path(config.results_path).joinpath('submission.csv')\n",
        "    test_preds_df.to_csv(p_submission, index=False)\n",
        "    logger_func(f'Test inference written to {p_submission}.')\n",
        "    return test_preds_df\n",
        "\n",
        "def train_K_folds_make_inference(config):\n",
        "    \"\"\"\n",
        "    Train K folds of the training dataset with out-of-fold validation.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        check_save_config(config)\n",
        "    except FileExistsError:\n",
        "        print(f\"Results path: {config.results_path} exists and not empty. Quit.\")\n",
        "        return\n",
        "\n",
        "    logger = get_logger(config.name, log_path=config.results_path)\n",
        "    logger.info(f\"{config.description}\")\n",
        "    logger.info(f\"Results path: {config.results_path}\")\n",
        "\n",
        "    data = Data(config)\n",
        "    kf = KFold(n_splits=config.K_fold, shuffle=True, random_state=config.seed)\n",
        "    oof_models, oof_model_paths, oof_labels, oof_preds, k_fold_scores = [], [], [], [], []\n",
        "    for k, (train_idx, val_idx) in enumerate(kf.split(data.train_files)):\n",
        "        if config.train_folds and k not in config.train_folds:\n",
        "            continue\n",
        "        logger.info(f\"--- Train fold {k} of {config.train_folds} ---\")\n",
        "        logger.info(f\"Train: {train_idx}  Val: {val_idx}\")\n",
        "        train_data = data.get_dataset(train_or_test=True, shuffle=True, file_indices=train_idx)\n",
        "        val_data = data.get_dataset(train_or_test=True, shuffle=False, file_indices=val_idx)\n",
        "        model, model_path = train(config, train_data, val_data, fold_k=k)\n",
        "        oof_models.append(model)\n",
        "        oof_model_paths.append(model_path)\n",
        "        # oof prediction\n",
        "        val_y_true, val_y_pred, val_score = oof_predict(model, val_data)\n",
        "        oof_labels.append(val_y_true)\n",
        "        oof_preds.append(val_y_pred)\n",
        "        k_fold_scores.append(val_score)\n",
        "        logger.info(f\"Fold {k} val score: {val_score}\")\n",
        "    oof_labels = tf.concat(oof_labels, axis=0)\n",
        "    oof_preds = tf.concat(oof_preds, axis=0)\n",
        "    oof_score = get_score(oof_labels, oof_preds)\n",
        "    logger.info(f\"OOF val score: {oof_score}\")\n",
        "\n",
        "    logger.info(\"--- Inference ---\")\n",
        "    test_preds_df = make_inference(config, oof_models, logger)\n",
        "    \n",
        "    return oof_models, oof_model_paths, oof_score, k_fold_scores, test_preds_df\n",
        "\n",
        "def load_results_make_inference(results_path: str):\n",
        "    \"\"\"\n",
        "    Load saved models and make inference on test dataset.\n",
        "    \"\"\"\n",
        "    p = Path(results_path).expanduser().resolve()\n",
        "    if not p.exists():\n",
        "        print(f\"Invalid results_path: {results_path}\")\n",
        "        return\n",
        "    config = joblib.load(p.joinpath('config.pkl'))\n",
        "    models = []\n",
        "    for k in config.train_folds or range(config.K_fold):\n",
        "        checkpoint_k = p.joinpath(f'checkpoint_fold{k}')\n",
        "        model = keras.models.load_model(checkpoint_k)\n",
        "        models.append(model)\n",
        "    test_preds_df = make_inference(config, models)\n",
        "\n",
        "    return test_preds_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3 Configuration <a class=\"anchor\" id=\"configuration\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALg4a4g_INxt"
      },
      "outputs": [],
      "source": [
        "config = Config(\n",
        "    name = \"CNN1d_GeM_AdamW_Bandpass_SameScaling_5Folds_\" + datetime.now().strftime(\"%Y-%m-%d_%H-%M\"),\n",
        "    description = \"CNN1d GeM AdamW CosineDecay, 5 folds, 8 epochs, batch_size 50. Same scaling for 3 channels.\",\n",
        "\n",
        "    # paths\n",
        "    # data_path = \"data/\",  # Dataset on local drive\n",
        "    # data_path = \"/content/drive/Shareddrives/ml/g2net/data/\",  # Dataset on Google Drive\n",
        "    data_path = \"gs://kds-8a5a5ceed201023b7b0d1880950ccc33c21b9bef067a7abe0dfb4aaa/\",  # Dataset on Kaggle GCS\n",
        "    # results_path = \"results/\",  # Save results on local drive\n",
        "    results_parent_path = \"/content/drive/Shareddrives/ml/g2net/results/\",  # Save results on Google Drive\n",
        "    results_path = None,  # results_parent_path + name, to be initialized in check_and_save_config().\n",
        "    shuffle_buf_size = 2048,\n",
        "\n",
        "    # train/test paras\n",
        "    K_fold = 5,\n",
        "    train_folds = [],  # If not empty, only train a subset of folds.\n",
        "    batch_size = 50,\n",
        "    epochs = 8,\n",
        "\n",
        "    # warm start\n",
        "    warm_start = False,\n",
        "    warm_start_model_path = \"\",\n",
        "\n",
        "    # algorithm/model paras\n",
        "    filter = dict(\n",
        "        scaling = 1e20,  # 1 / np.array([2e-20, 2e-20, 0.5e-20]).reshape(-1, 1),\n",
        "        filter_type = 'bandpass', filter_order=8, f_lo=25, f_hi=1000, sampling_rate=2048\n",
        "        #filter_type='highpass', filter_order=5, f_lo=20.43, f_hi=None, sampling_rate=2048\n",
        "    ),\n",
        "    lr_schedule = 'CosineDecay',\n",
        "    lr_schedule_paras = {'CosineDecay': dict(initial_learning_rate=1e-4, decay_steps=5, alpha=1e-3)},  # init_lr=eta_max alpha= eta_min/eta_max\n",
        "    optimizer = 'AdamW',\n",
        "    optimizer_paras = {\n",
        "        'SGDW': dict(weight_decay=1e-5, momentum=0.9, nesterov=True),\n",
        "        'AdamW': dict(weight_decay=1e-5),\n",
        "    },\n",
        "\n",
        "    # misc\n",
        "    seed = 42,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4 Run training and inference <a class=\"anchor\" id=\"run-training-inference\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IxFVT2a4Ba_B",
        "outputId": "611ce88f-99f6-42e7-cb28-d769d366ccd6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-01-27 09:22:03,364 - INFO - CNN1d GeM AdamW CosineDecay, 5 folds, 8 epochs, batch_size 50. Same scaling for 3 channels.\n",
            "2022-01-27 09:22:03,366 - INFO - Results path: /content/drive/Shareddrives/ml/g2net/results/CNN1d_GeM_AdamW_Bandpass_SameScaling_5Folds_2022-01-27_09-22\n",
            "2022-01-27 09:22:03,377 - INFO - --- Train fold 0 of [] ---\n",
            "2022-01-27 09:22:03,379 - INFO - Train: [ 2  3  4  5  6  7  8  9 10 11 12 13 14 16 18 19]  Val: [ 0  1 15 17]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "8960/8960 [==============================] - 884s 97ms/step - loss: 0.4637 - AUC: 0.8409 - val_loss: 0.4317 - val_AUC: 0.8647\n",
            "Epoch 2/8\n",
            "8960/8960 [==============================] - 732s 81ms/step - loss: 0.4304 - AUC: 0.8602 - val_loss: 0.4276 - val_AUC: 0.8668\n",
            "Epoch 3/8\n",
            "8960/8960 [==============================] - 718s 80ms/step - loss: 0.4205 - AUC: 0.8660 - val_loss: 0.4405 - val_AUC: 0.8686\n",
            "Epoch 4/8\n",
            "8960/8960 [==============================] - 775s 86ms/step - loss: 0.4129 - AUC: 0.8704 - val_loss: 0.4185 - val_AUC: 0.8687\n",
            "Epoch 5/8\n",
            "8960/8960 [==============================] - 705s 79ms/step - loss: 0.4061 - AUC: 0.8747 - val_loss: 0.4517 - val_AUC: 0.8669\n",
            "Epoch 6/8\n",
            "8960/8960 [==============================] - 703s 78ms/step - loss: 0.3973 - AUC: 0.8801 - val_loss: 0.4350 - val_AUC: 0.8642\n",
            "Epoch 7/8\n",
            "8960/8960 [==============================] - 688s 77ms/step - loss: 0.3870 - AUC: 0.8865 - val_loss: 0.4525 - val_AUC: 0.8575\n",
            "Epoch 8/8\n",
            "8960/8960 [==============================] - 699s 78ms/step - loss: 0.3745 - AUC: 0.8940 - val_loss: 0.4485 - val_AUC: 0.8546\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-01-27 11:02:36,575 - INFO - Fold 0 val score: 0.8686988353729248\n",
            "2022-01-27 11:02:36,577 - INFO - --- Train fold 1 of [] ---\n",
            "2022-01-27 11:02:36,578 - INFO - Train: [ 0  1  2  4  6  7  9 10 12 13 14 15 16 17 18 19]  Val: [ 3  5  8 11]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "8960/8960 [==============================] - 698s 77ms/step - loss: 0.4645 - AUC: 0.8407 - val_loss: 0.4285 - val_AUC: 0.8627\n",
            "Epoch 2/8\n",
            "8960/8960 [==============================] - 674s 75ms/step - loss: 0.4301 - AUC: 0.8604 - val_loss: 0.4388 - val_AUC: 0.8643\n",
            "Epoch 3/8\n",
            "8960/8960 [==============================] - 671s 75ms/step - loss: 0.4196 - AUC: 0.8666 - val_loss: 0.4346 - val_AUC: 0.8665\n",
            "Epoch 4/8\n",
            "8960/8960 [==============================] - 664s 74ms/step - loss: 0.4121 - AUC: 0.8712 - val_loss: 0.4241 - val_AUC: 0.8650\n",
            "Epoch 5/8\n",
            "8960/8960 [==============================] - 663s 74ms/step - loss: 0.4048 - AUC: 0.8757 - val_loss: 0.4277 - val_AUC: 0.8630\n",
            "Epoch 6/8\n",
            "8960/8960 [==============================] - 666s 74ms/step - loss: 0.3953 - AUC: 0.8816 - val_loss: 0.4344 - val_AUC: 0.8599\n",
            "Epoch 7/8\n",
            "8960/8960 [==============================] - 665s 74ms/step - loss: 0.3842 - AUC: 0.8884 - val_loss: 0.4794 - val_AUC: 0.8533\n",
            "Epoch 8/8\n",
            "8960/8960 [==============================] - 665s 74ms/step - loss: 0.3718 - AUC: 0.8960 - val_loss: 0.4691 - val_AUC: 0.8504\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-01-27 12:34:16,206 - INFO - Fold 1 val score: 0.8664733171463013\n",
            "2022-01-27 12:34:16,207 - INFO - --- Train fold 2 of [] ---\n",
            "2022-01-27 12:34:16,210 - INFO - Train: [ 0  1  3  4  5  6  7  8  9 10 11 12 14 15 17 19]  Val: [ 2 13 16 18]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "8960/8960 [==============================] - 673s 75ms/step - loss: 0.4609 - AUC: 0.8431 - val_loss: 0.4436 - val_AUC: 0.8609\n",
            "Epoch 2/8\n",
            "8960/8960 [==============================] - 672s 75ms/step - loss: 0.4290 - AUC: 0.8610 - val_loss: 0.4243 - val_AUC: 0.8651\n",
            "Epoch 3/8\n",
            "8960/8960 [==============================] - 690s 77ms/step - loss: 0.4184 - AUC: 0.8675 - val_loss: 0.4250 - val_AUC: 0.8652\n",
            "Epoch 4/8\n",
            "8960/8960 [==============================] - 678s 75ms/step - loss: 0.4107 - AUC: 0.8721 - val_loss: 0.4320 - val_AUC: 0.8645\n",
            "Epoch 5/8\n",
            "8960/8960 [==============================] - 685s 76ms/step - loss: 0.4032 - AUC: 0.8766 - val_loss: 0.4313 - val_AUC: 0.8631\n",
            "Epoch 6/8\n",
            "8960/8960 [==============================] - 699s 78ms/step - loss: 0.3947 - AUC: 0.8819 - val_loss: 0.4505 - val_AUC: 0.8604\n",
            "Epoch 7/8\n",
            "8960/8960 [==============================] - 663s 74ms/step - loss: 0.3842 - AUC: 0.8884 - val_loss: 0.4925 - val_AUC: 0.8563\n",
            "Epoch 8/8\n",
            "8960/8960 [==============================] - 685s 76ms/step - loss: 0.3722 - AUC: 0.8958 - val_loss: 0.4723 - val_AUC: 0.8527\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-01-27 14:06:56,342 - INFO - Fold 2 val score: 0.8652385473251343\n",
            "2022-01-27 14:06:56,344 - INFO - --- Train fold 3 of [] ---\n",
            "2022-01-27 14:06:56,347 - INFO - Train: [ 0  1  2  3  5  6  7  8 10 11 13 14 15 16 17 18]  Val: [ 4  9 12 19]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "8960/8960 [==============================] - 706s 78ms/step - loss: 0.4647 - AUC: 0.8406 - val_loss: 0.4329 - val_AUC: 0.8633\n",
            "Epoch 2/8\n",
            "8960/8960 [==============================] - 666s 74ms/step - loss: 0.4301 - AUC: 0.8604 - val_loss: 0.4400 - val_AUC: 0.8650\n",
            "Epoch 3/8\n",
            "8960/8960 [==============================] - 681s 76ms/step - loss: 0.4191 - AUC: 0.8669 - val_loss: 0.4378 - val_AUC: 0.8666\n",
            "Epoch 4/8\n",
            "8960/8960 [==============================] - 674s 75ms/step - loss: 0.4114 - AUC: 0.8715 - val_loss: 0.4905 - val_AUC: 0.8649\n",
            "Epoch 5/8\n",
            "8960/8960 [==============================] - 681s 76ms/step - loss: 0.4037 - AUC: 0.8760 - val_loss: 0.4347 - val_AUC: 0.8626\n",
            "Epoch 6/8\n",
            "8960/8960 [==============================] - 690s 77ms/step - loss: 0.3951 - AUC: 0.8817 - val_loss: 0.4350 - val_AUC: 0.8591\n",
            "Epoch 7/8\n",
            "8960/8960 [==============================] - 721s 80ms/step - loss: 0.3844 - AUC: 0.8884 - val_loss: 0.4616 - val_AUC: 0.8546\n",
            "Epoch 8/8\n",
            "8960/8960 [==============================] - 749s 83ms/step - loss: 0.3710 - AUC: 0.8967 - val_loss: 0.4671 - val_AUC: 0.8496\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-01-27 15:42:21,742 - INFO - Fold 3 val score: 0.8666242957115173\n",
            "2022-01-27 15:42:21,745 - INFO - --- Train fold 4 of [] ---\n",
            "2022-01-27 15:42:21,746 - INFO - Train: [ 0  1  2  3  4  5  8  9 11 12 13 15 16 17 18 19]  Val: [ 6  7 10 14]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n",
            "8960/8960 [==============================] - 730s 81ms/step - loss: 0.4625 - AUC: 0.8418 - val_loss: 0.4302 - val_AUC: 0.8640\n",
            "Epoch 2/8\n",
            "8960/8960 [==============================] - 692s 77ms/step - loss: 0.4295 - AUC: 0.8611 - val_loss: 0.4244 - val_AUC: 0.8668\n",
            "Epoch 3/8\n",
            "8960/8960 [==============================] - 680s 76ms/step - loss: 0.4196 - AUC: 0.8667 - val_loss: 0.4211 - val_AUC: 0.8666\n",
            "Epoch 4/8\n",
            "8960/8960 [==============================] - 690s 77ms/step - loss: 0.4118 - AUC: 0.8714 - val_loss: 0.4417 - val_AUC: 0.8665\n",
            "Epoch 5/8\n",
            "8960/8960 [==============================] - 680s 76ms/step - loss: 0.4045 - AUC: 0.8760 - val_loss: 0.4302 - val_AUC: 0.8648\n",
            "Epoch 6/8\n",
            "8960/8960 [==============================] - 684s 76ms/step - loss: 0.3963 - AUC: 0.8811 - val_loss: 0.4618 - val_AUC: 0.8617\n",
            "Epoch 7/8\n",
            "8960/8960 [==============================] - 679s 76ms/step - loss: 0.3856 - AUC: 0.8877 - val_loss: 0.5145 - val_AUC: 0.8560\n",
            "Epoch 8/8\n",
            "8960/8960 [==============================] - 683s 76ms/step - loss: 0.3730 - AUC: 0.8956 - val_loss: 0.5011 - val_AUC: 0.8520\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-01-27 17:16:26,068 - INFO - Fold 4 val score: 0.8668286204338074\n",
            "2022-01-27 17:16:26,080 - INFO - OOF val score: 0.8647056221961975\n",
            "2022-01-27 17:16:26,081 - INFO - --- Inference ---\n"
          ]
        }
      ],
      "source": [
        "oof_models, oof_model_paths, oof_score, k_fold_scores, test_preds_df = train_K_folds_make_inference(config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsWyHCCVGi6Q"
      },
      "source": [
        "# 5 Offline inference from multiple runs of folds <a class=\"anchor\" id=\"offline-inference\">\n",
        "Colab has running time limits so have to run multiple folds separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfHQHAm3EZYO",
        "outputId": "ab9f6d1e-b7cc-4ee7-b1bb-2d63aa813ea7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Make inference by fold 0 model\n",
            "Make inference by fold 1 model\n",
            "Make inference by fold 2 model\n",
            "Make inference by fold 3 model\n",
            "Make inference by fold 4 model\n",
            "Test inference written to /content/drive/Shareddrives/ml/g2net/results/CNN1d_GeM_AdamW_Bandpass_SameScaling_5Folds_2022-01-27_09-22/submission.csv.\n"
          ]
        }
      ],
      "source": [
        "results_path = \"/content/drive/Shareddrives/ml/g2net/results/CNN1d_GeM_AdamW_Bandpass_SameScaling_5Folds_2022-01-27_09-22\"\n",
        "test_preds_df = load_results_make_inference(results_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-cmAqtzJ9Mc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "G2Net_CNN1D_GeM_AdamW_Bandpass",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
